{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b403856f-ec6b-408f-8457-c442233c1c9f",
   "metadata": {},
   "source": [
    "# Two-level cross-validation on the logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa8c6d-f083-4701-a306-fd0a2cb34df5",
   "metadata": {},
   "source": [
    "### Date: 2023-10-31\n",
    "### Author: Emma Louise Blair (s214680)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a45164-8e24-43ae-992a-2b105f455071",
   "metadata": {},
   "source": [
    "The purpose of the two-level cross-validation on the logistic regression model is to analyze how well the model is able to categorize whether an individual has a yearly income below or above 50K/y dollars. We used a 10-fold cross-validation and hyperparameter tuning to optimize the model by minimizing the test error value for each fold. For a logistic regression model the hyperparameter in question is the L2 regularization parameter $\\lambda$. We use ```GridSearchCV()``` to find the best $\\lambda$-value based on the accuracy score for each $\\lambda$-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a40c1b-e34c-4d43-8e93-555020c13dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab7ff4d-ece0-4080-bbff-2ba1eec9d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_df = pd.read_csv(\"../2023-10-05_jennifer_data_preparation/independent_train.csv\")\n",
    "y_df = pd.read_csv(\"../2023-10-05_jennifer_data_preparation/dependent_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19fb94ac-a135-41e9-beff-36d1a5d3b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten dependent variables (y) to 1D array\n",
    "y_df = y_df.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6c37b7-cd9e-4349-821f-00fd11fec419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables and lists\n",
    "K = 10\n",
    "kfold = KFold(n_splits=K)\n",
    "\n",
    "best_lambdas = []\n",
    "E_test_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "547c74e0-08a0-4a8f-82ec-f75e6a6cce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer loop\n",
    "for train_idx, test_idx in kfold.split(X_df, y_df):\n",
    "    X_train_outer, X_test_outer = X_df.iloc[train_idx], X_df.iloc[test_idx]\n",
    "    y_train_outer, y_test_outer = y_df[train_idx], y_df[test_idx]\n",
    "    \n",
    "    param_grid = {'C': np.logspace(-4, 4, 50)}\n",
    "    inner_model = LogisticRegression(max_iter=1000)\n",
    "    grid_search = GridSearchCV(inner_model, param_grid, cv=K, scoring='accuracy')\n",
    "    grid_search.fit(X_train_outer, y_train_outer)\n",
    "    \n",
    "    # Determine best lambda value\n",
    "    best_lambda = grid_search.best_params_\n",
    "    \n",
    "    # Train model with best lambda value\n",
    "    outer_model = LogisticRegression(max_iter=1000, **best_lambda)\n",
    "    outer_model.fit(X_train_outer, y_train_outer)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    y_pred_outer = outer_model.predict(X_test_outer)\n",
    "    accuracy_outer = accuracy_score(y_test_outer, y_pred_outer)\n",
    "    \n",
    "    # Store best lambda value and corresponding E^test value (E^test = 1 - accuracy)\n",
    "    best_lambdas.append(best_lambda)\n",
    "    E_test_values.append(1 - accuracy_outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e77428d4-452e-43f2-ab4c-6932a14b06c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda value for each fold in the model: ['0.060', '2.560', '0.041', '0.060', '0.569', '1.758', '0.569', '0.569', '5.429', '24.421', '0.060', '2.560', '0.041', '0.060', '0.569', '1.758', '0.569', '0.569', '5.429', '24.421']\n",
      "E^test value for each fold in the model: ['0.231', '0.192', '0.308', '0.346', '0.154', '0.231', '0.240', '0.280', '0.280', '0.320', '0.231', '0.192', '0.308', '0.346', '0.154', '0.231', '0.240', '0.280', '0.280', '0.320']\n"
     ]
    }
   ],
   "source": [
    "print(\"Best lambda value for each fold in the model:\", [f'{x[\"C\"]:.3f}' for x in best_lambdas])\n",
    "print(\"E^test value for each fold in the model:\", [f'{x:.3f}' for x in E_test_values])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
